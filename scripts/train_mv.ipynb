{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from clort.data import ArgoCL, ArgoCl_collate_fxn\n",
    "from clort.model import ContrastiveLoss, MemoryBank, MemoryBankInfer, MultiViewEncoder\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pkby3ugr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">colorful-shape-17</strong> at: <a href='https://wandb.ai/shivampr21/CLORT/runs/pkby3ugr' target=\"_blank\">https://wandb.ai/shivampr21/CLORT/runs/pkby3ugr</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230620_115908-pkby3ugr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pkby3ugr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33008601d85c459c8568231d361cafa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669268050001542, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shivampr21/Research/MOT-Research/CLORT/scripts/wandb/run-20230620_115919-17azbb5p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shivampr21/CLORT/runs/17azbb5p' target=\"_blank\">comic-paper-18</a></strong> to <a href='https://wandb.ai/shivampr21/CLORT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shivampr21/CLORT' target=\"_blank\">https://wandb.ai/shivampr21/CLORT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shivampr21/CLORT/runs/17azbb5p' target=\"_blank\">https://wandb.ai/shivampr21/CLORT/runs/17azbb5p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start a new wandb run to track this script\n",
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"CLORT\",\n",
    "    \n",
    "    resume=False,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"architecture\": \"Multi_View Encoder : Renet Single View Encoder\",\n",
    "    \"training data\": [\"train4\"],\n",
    "    \"validation data\": [\"train4\"],\n",
    "    \"saved model\": None,\n",
    "    \"batch size\": 1,\n",
    "    \"temporal horizon\": 1,\n",
    "    \"temporal overlap\": 0,\n",
    "    \"max objects\": None,\n",
    "    \"static contrast\": True,\n",
    "    \"Normalization\": \"None\",\n",
    "    \"Normalization momemtum\": 0.1,\n",
    "    \"n_epochs\": 30\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root: str = \"../../../datasets/argoverse-tracking/argov1_proc/\"\n",
    "train_splits: List[str] = run.config[\"training data\"]\n",
    "val_splits: List[str] = run.config[\"validation data\"]\n",
    "model_save_dir: str = '/home/shivam/CLORT_MV/'\n",
    "load_saved_model: str | None = run.config[\"saved model\"]\n",
    "batch_size: int = run.config[\"batch size\"]\n",
    "max_objects: int = run.config[\"max objects\"]\n",
    "th: int = run.config[\"temporal horizon\"]\n",
    "to: int = run.config[\"temporal overlap\"]\n",
    "nw: int = 0\n",
    "model_device: torch.device | str = 'cuda'\n",
    "memory_device: torch.device | str = 'cpu'\n",
    "static_contrast: bool = run.config[\"static contrast\"]\n",
    "n_epochs: int = run.config['n_epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ArgoCL(root,\n",
    "                       temporal_horizon=th,\n",
    "                       temporal_overlap=to,\n",
    "                       max_objects=max_objects,\n",
    "                       distance_threshold=(0, 50),\n",
    "                       splits=train_splits, img_size=(224, 224),\n",
    "                       point_cloud_size=[20, 50, 100, 250, 500, 1000, 1500],\n",
    "                       in_global_frame=True, pivot_to_first_frame=True,\n",
    "                       image=True, pcl=True, bbox=True)\n",
    "\n",
    "val_dataset = ArgoCL(root,\n",
    "                    temporal_horizon=th,\n",
    "                    temporal_overlap=to,\n",
    "                    distance_threshold=(0, 50),\n",
    "                    splits=val_splits, img_size=(224, 224),\n",
    "                    point_cloud_size=[20, 50, 100, 250, 500, 1000, 1500],\n",
    "                    in_global_frame=True, pivot_to_first_frame=True,\n",
    "                    image=True, pcl=True, bbox=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True,\n",
    "                    collate_fn=ArgoCl_collate_fxn, num_workers=nw)\n",
    "\n",
    "val_dl = DataLoader(val_dataset, 1, shuffle=False,\n",
    "                collate_fn=ArgoCl_collate_fxn, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_enc = MultiViewEncoder(out_dim=n_features)\n",
    "mv_enc = mv_enc.to(model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = MemoryBank(train_dataset.n_tracks, n_features, 5,\n",
    "                    alpha=torch.tensor([0.5, 0.4, 0.3, 0.2, 0.1], dtype=torch.float32, device=memory_device),\n",
    "                    device=memory_device)\n",
    "\n",
    "cl = ContrastiveLoss(temp=0.05, static_contrast=static_contrast)\n",
    "\n",
    "mb_infer = MemoryBank(val_dataset.n_tracks, n_features, 5,\n",
    "                    alpha=torch.tensor([0.5, 0.4, 0.3, 0.2, 0.1], dtype=torch.float32, device=memory_device),\n",
    "                    device=memory_device)\n",
    "\n",
    "cl_infer = ContrastiveLoss(temp=0.05, static_contrast=static_contrast)\n",
    "\n",
    "mb_primed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "                        params=[\n",
    "                            {'params' : mv_enc.sv_enc1.parameters(), 'lr': 1e-4, \"weight_decay\":1e-3},\n",
    "                            {'params': mv_enc.sv_enc2.parameters(), 'lr': 1e-5, \"weight_decay\":1e-4},\n",
    "                            {'params': mv_enc.sv_enc3.parameters(), 'lr': 1e-5, \"weight_decay\":1e-4},\n",
    "                            {'params': mv_enc.gat.parameters(), 'lr': 1e-5, \"weight_decay\":1e-4},\n",
    "                            {'params': mv_enc.projection_head.parameters(), 'lr': 1e-5, \"weight_decay\":1e-4}\n",
    "                            ], lr = 1e-4, weight_decay=1e-3\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from file\n",
    "last_epoch = -1\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1, last_epoch=last_epoch)\n",
    "\n",
    "training_loss: List[float] = []\n",
    "validation_loss: List[float] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb.run.resumed:\n",
    "    print(f'Loading model from file: {load_saved_model = }')\n",
    "    ckpt = torch.load(wandb.restore(load_saved_model))\n",
    "    mv_enc.load_state_dict(ckpt['mv_enc'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    lr_scheduler.load_state_dict(ckpt['lr_scheduler'])\n",
    "    training_loss = ckpt['train_loss']\n",
    "    validation_loss = ckpt['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, mv_enc, train_dl, optimizer, criterion, mem_bank, log_step=100, mb_priming = False):\n",
    "    mv_enc.train() # Enable training\n",
    "    \n",
    "    training_loss = []\n",
    "    \n",
    "    # Training loop\n",
    "    for itr, (_, _, imgs, imgs_sz, _, track_idxs, _, _, _) in (t_bar := tqdm(enumerate(train_dl))):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # pcls = pcls.to(model_device)\n",
    "        imgs = imgs.to(model_device)\n",
    "        track_idxs = torch.from_numpy(track_idxs.astype(np.int32))\n",
    "        # bboxs = bboxs.to(model_device)\n",
    "\n",
    "        mv_e = mv_enc(imgs, imgs_sz)\n",
    "        \n",
    "        if mb_priming:\n",
    "            t_bar.set_description('Priming')\n",
    "            \n",
    "            loss = criterion(mv_e, track_idxs, mem_bank.get_memory())\n",
    "            loss.backward() # clear_graph\n",
    "            \n",
    "            mem_bank.update(mv_e.detach().cpu(), track_idxs) # Update memory bank\n",
    "            continue\n",
    "        \n",
    "        loss = criterion(mv_e, track_idxs, mem_bank.get_memory())\n",
    "        training_loss.append(loss.numpy(force=True).item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        mem_bank.update(mv_e.detach().cpu(), track_idxs) # Update memory bank\n",
    "        \n",
    "        #t_bar.set_description(f'{epoch = } and {itr = } : Mean Training loss : {np.mean(training_loss[-los_step if itr>los_step else -(itr+1):])}')\n",
    "        \n",
    "        if itr%log_step == log_step-1:\n",
    "            t_bar.set_description(f'{epoch = } and {itr = } : Mean Training loss : {np.mean(training_loss[-log_step:])}')\n",
    "\n",
    "            wandb.log({'epoch': epoch+1, 'itr': itr+1,\n",
    "                        'Training Loss': np.mean(training_loss[-log_step:])\n",
    "                      })\n",
    "        \n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch, mv_enc, train_dl, criterion, mem_bank, log_step=100):\n",
    "    mv_enc.eval() # Enable training\n",
    "    \n",
    "    validation_loss = []\n",
    "    \n",
    "    # Validation loop\n",
    "    with torch.no_grad():\n",
    "        for itr, (_, _, imgs, imgs_sz, _, track_idxs, _, _, _) in (v_bar := tqdm(enumerate(val_dl))):\n",
    "            # pcls = pcls.to(model_device)\n",
    "            imgs = imgs.to(model_device)\n",
    "            track_idxs = torch.from_numpy(track_idxs.astype(np.int32))\n",
    "            # bboxs = bboxs.to(model_device)\n",
    "\n",
    "            mv_e = mv_enc(imgs, imgs_sz)\n",
    "\n",
    "            loss = criterion(mv_e, track_idxs, mem_bank.get_memory())\n",
    "            validation_loss.append(loss.numpy(force=True).item())\n",
    "\n",
    "            mem_bank.update(mv_e.detach().cpu(), track_idxs) # Update memory bank\n",
    "\n",
    "            #v_bar.set_description(f'{epoch = } and {itr = } : Mean Training loss : {np.mean(training_loss[-100 if itr>100 else -(itr+1):])}')\n",
    "\n",
    "            if itr%(log_step) == log_step-1:\n",
    "                v_bar.set_description(f'{epoch = } and {itr = } : Mean Validation loss : {np.mean(validation_loss[-log_step:])}')\n",
    "\n",
    "                wandb.log({'Epoch': epoch+1, 'Iteration': itr+1,\n",
    "                            'Validation Loss': np.mean(validation_loss[-log_step:])\n",
    "                          })\n",
    "        \n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = lr_scheduler.last_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for module in mv_enc.modules():\n",
    "#     print(f'{len(list(module.modules())) = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for child in mv_enc.sv_enc1.children():\n",
    "#     for child_ in child.children():\n",
    "#         for child__ in child_.children():\n",
    "#             for child___ in child__.children():\n",
    "#                 if type(child___) == nn.BatchNorm2d:\n",
    "#                     child___.track_running_stats = False\n",
    "#                     print(f'{child___ = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for child in mv_enc.sv_enc2.children():\n",
    "#     if type(child) == nn.LayerNorm:\n",
    "#         print(dict(child.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Priming: : 914it [03:09,  4.83it/s]\n",
      "/home/shivampr21/.venv/clort/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "epoch = 1 and itr = 899 : Mean Training loss : 2.063762639909983: : 914it [03:35,  4.24it/s] \n",
      "epoch = 2 and itr = 899 : Mean Training loss : 1.5817130291834474: : 914it [03:24,  4.46it/s]\n",
      "epoch = 3 and itr = 899 : Mean Training loss : 2.0123099377006293: : 914it [03:22,  4.50it/s]\n",
      "epoch = 4 and itr = 899 : Mean Training loss : 2.627462146356702: : 914it [03:28,  4.39it/s] \n",
      "epoch = 5 and itr = 899 : Mean Training loss : 2.082427296638489: : 914it [03:21,  4.54it/s] \n",
      "epoch = 6 and itr = 899 : Mean Training loss : 1.9670758653990925: : 914it [03:40,  4.14it/s]\n",
      "epoch = 7 and itr = 199 : Mean Training loss : 2.374086647182703: : 270it [00:59,  4.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(last_epoch, n_epochs):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     model_path = os.path.join(model_save_dir, f'model.pth')\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmv_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmb_priming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m###################################################################################\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m### Validation loss\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, mv_enc, train_dl, optimizer, criterion, mem_bank, log_step, mb_priming)\u001b[0m\n\u001b[1;32m      4\u001b[0m training_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itr, (_, _, imgs, imgs_sz, _, track_idxs, _, _, _) \u001b[38;5;129;01min\u001b[39;00m (t_bar \u001b[38;5;241m:=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_dl))):\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# pcls = pcls.to(model_device)\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.10/site-packages/torch/utils/data/dataloader.py:635\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.10/site-packages/torch/utils/data/dataloader.py:679\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    678\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    681\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Research/MOT-Research/CLORT/clort/data/ArgoCL.py:260\u001b[0m, in \u001b[0;36mArgoCL.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    257\u001b[0m     pcls \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mconcatenate(pcls, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m (pivot \u001b[38;5;28;01mif\u001b[39;00m pivot \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)) \u001b[38;5;66;03m# Concatenate on points dimension\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim:\n\u001b[0;32m--> 260\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.\u001b[39m) \u001b[38;5;66;03m# Concatenate images on channel dimension # [_, 250, 250]\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, imgs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], imgs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# torch.cat(\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m#     imgs.unsqueeze(dim=0).split(split_size=3, dim=1)   # ([1, 3, 250, 250]...)\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m#     , dim=0)                                           # [_//3, 3, 250, 250] # dimension 1 is number of views which\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m#                                                                                                 # can be extracted with $imgs_sz$ split\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not mb_primed:\n",
    "    train(-1, mv_enc, train_dl, optimizer, cl, mb, log_step=100, mb_priming=True)\n",
    "    mb_primed = True\n",
    "\n",
    "for epoch in range(last_epoch, n_epochs):\n",
    "    model_path = os.path.join(model_save_dir, f'model.pth')\n",
    "    \n",
    "    train_loss = train(epoch, mv_enc, train_dl, optimizer, cl, mb, log_step=100, mb_priming=False)\n",
    "    \n",
    "    ###################################################################################\n",
    "    ### Validation loss\n",
    "    if epoch%10 == 9:\n",
    "        val_loss = val(epoch, mv_enc, val_dl, cl_infer, mb_infer, log_step=100) \n",
    "    ### Validation loss\n",
    "    ###################################################################################\n",
    "    lr_scheduler.step() # Step Learning rate\n",
    "\n",
    "    model_info = {\n",
    "        'EPOCH': epoch,\n",
    "        'mv_enc': mv_enc.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'train_loss': training_loss,\n",
    "        'val_loss': validation_loss\n",
    "    }\n",
    "    \n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "    torch.save(model_info, model_path)\n",
    "    \n",
    "    wandb.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a45d48c5d8437f8f82e08eb48c7e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>█▂▃▇▄▆▄▆▆▆▅▄▃▄▄▆▆▃▆▃▁▅▄▆▅▇▄▄▃▄▅▇▃▆▂▄▅▆▃▅</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>itr</td><td>▁▂▃▅▅▇█▁▃▄▅▆▇▁▂▄▅▅▇█▂▃▅▅▆█▁▃▄▅▆▇▁▂▃▅▅▇█▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>2.37409</td></tr><tr><td>epoch</td><td>8</td></tr><tr><td>itr</td><td>200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comic-paper-18</strong> at: <a href='https://wandb.ai/shivampr21/CLORT/runs/17azbb5p' target=\"_blank\">https://wandb.ai/shivampr21/CLORT/runs/17azbb5p</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230620_115919-17azbb5p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
