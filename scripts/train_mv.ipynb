{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivampr21/.venv/clort/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libnvjpeg.so.11: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from clort import ArgoCL, ArgoCl_collate_fxn\n",
    "from clort.model import ContrastiveLoss, MemoryBank, MemoryBankInfer, MultiViewEncoder\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"CLORT\",\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"architecture\": \"Multi_View Encoder\",\n",
    "#     \"dataset\": \"ArgoCL : Train1\",\n",
    "#     \"epochs\": 30,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root: str = \"../../../datasets/argoverse-tracking/argov1_proc/\"\n",
    "splits: List[str] = ['train4']\n",
    "model_save_dir: str = '~/.tmp/CLORT/'\n",
    "load_saved_model: str | None = None\n",
    "batch_size: int = 1\n",
    "th: int = 1\n",
    "to: int = 0\n",
    "nw: int = 0\n",
    "model_device: torch.device | str = 'cuda'\n",
    "memory_device: torch.device | str = 'cpu'\n",
    "static_contrast: bool = False\n",
    "n_epochs: int = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ArgoCL(root,\n",
    "                       temporal_horizon=th,\n",
    "                       temporal_overlap=to,\n",
    "                       distance_threshold=(0, 100),\n",
    "                       splits=splits, img_size=(224, 224),\n",
    "                       point_cloud_size=[20, 50, 100, 250, 500, 1000, 1500],\n",
    "                       in_global_frame=True, pivot_to_first_frame=True,\n",
    "                       image=True, pcl=True, bbox=True)\n",
    "\n",
    "val_dataset = ArgoCL(root,\n",
    "                    temporal_horizon=1,\n",
    "                    temporal_overlap=0,\n",
    "                    distance_threshold=(0, 100),\n",
    "                    splits=['val'], img_size=(224, 224),\n",
    "                    point_cloud_size=[20, 50, 100, 250, 500, 1000, 1500],\n",
    "                    in_global_frame=True, pivot_to_first_frame=True,\n",
    "                    image=True, pcl=True, bbox=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=False,\n",
    "                    collate_fn=ArgoCl_collate_fxn, num_workers=nw)\n",
    "\n",
    "val_dl = DataLoader(val_dataset, 1, shuffle=False,\n",
    "                collate_fn=ArgoCl_collate_fxn, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_enc = MultiViewEncoder(out_dim=n_features)\n",
    "mv_enc = mv_enc.to(model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = MemoryBank(train_dataset.n_tracks, n_features, 5,\n",
    "                    alpha=torch.tensor([0.5, 0.4, 0.3, 0.2, 0.1], dtype=torch.float32, device=memory_device),\n",
    "                    device=memory_device)\n",
    "\n",
    "cl = ContrastiveLoss(temp=0.05, static_contrast=static_contrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "                        params=[\n",
    "                            {'params' : mv_enc.sv_enc1.parameters(), 'lr': 1e-6},\n",
    "                            {'params': mv_enc.sv_enc2.parameters(), 'lr': 1e-4},\n",
    "                            {'params': mv_enc.sv_enc3.parameters(), 'lr': 1e-4},\n",
    "                            {'params': mv_enc.gat.parameters(), 'lr': 1e-4},\n",
    "                            {'params': mv_enc.projection_head.parameters(), 'lr': 1e-4}\n",
    "                            ], lr = 1e-4, weight_decay=1e-3\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from file\n",
    "last_epoch = -1\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1, last_epoch=last_epoch)\n",
    "\n",
    "training_loss: List[float] = []\n",
    "validation_loss: List[float] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_saved_model is not None:\n",
    "    ckpt = torch.load(load_saved_model)\n",
    "    mv_enc.load_state_dict(ckpt['mv_enc'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    lr_scheduler.load_state_dict(ckpt['lr_scheduler'])\n",
    "    training_loss = ckpt['train_loss']\n",
    "    validation_loss = ckpt['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:03,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 and itr = 9 : Mean Training loss : 2.2776680469512938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "2it [00:00, 11.40it/s]\u001b[A\n",
      "4it [00:00, 11.15it/s]\u001b[A\n",
      "6it [00:00, 11.31it/s]\u001b[A\n",
      "8it [00:00, 11.61it/s]\u001b[A\n",
      "10it [00:00, 11.80it/s]\u001b[A\n",
      "12it [00:01, 12.08it/s]\u001b[A\n",
      "14it [00:01, 12.50it/s]\u001b[A\n",
      "16it [00:01, 12.77it/s]\u001b[A\n",
      "18it [00:01, 13.01it/s]\u001b[A\n",
      "20it [00:01, 13.54it/s]\u001b[A\n",
      "22it [00:01, 14.21it/s]\u001b[A\n",
      "24it [00:01, 14.86it/s]\u001b[A\n",
      "26it [00:01, 15.42it/s]\u001b[A\n",
      "28it [00:02, 14.94it/s]\u001b[A\n",
      "30it [00:02, 14.46it/s]\u001b[A\n",
      "32it [00:02, 14.07it/s]\u001b[A\n",
      "34it [00:02, 13.77it/s]\u001b[A\n",
      "36it [00:02, 13.34it/s]\u001b[A\n",
      "38it [00:02, 12.40it/s]\u001b[A\n",
      "40it [00:03, 11.31it/s]\u001b[A\n",
      "42it [00:03, 10.66it/s]\u001b[A\n",
      "44it [00:03, 10.26it/s]\u001b[A\n",
      "46it [00:03,  9.64it/s]\u001b[A\n",
      "47it [00:03,  9.37it/s]\u001b[A\n",
      "48it [00:04,  9.21it/s]\u001b[A\n",
      "49it [00:04,  8.96it/s]\u001b[A\n",
      "50it [00:04,  8.70it/s]\u001b[A\n",
      "51it [00:04,  8.73it/s]\u001b[A\n",
      "52it [00:04,  8.71it/s]\u001b[A\n",
      "53it [00:04,  8.78it/s]\u001b[A\n",
      "54it [00:04,  8.90it/s]\u001b[A\n",
      "55it [00:04,  8.95it/s]\u001b[A\n",
      "57it [00:05,  9.79it/s]\u001b[A\n",
      "59it [00:05,  9.81it/s]\u001b[A\n",
      "60it [00:05,  9.41it/s]\u001b[A\n",
      "61it [00:05,  9.50it/s]\u001b[A\n",
      "62it [00:05,  9.59it/s]\u001b[A\n",
      "63it [00:05,  9.62it/s]\u001b[A\n",
      "64it [00:05,  9.64it/s]\u001b[A\n",
      "65it [00:05,  9.45it/s]\u001b[A\n",
      "66it [00:05,  9.38it/s]\u001b[A\n",
      "67it [00:06,  9.30it/s]\u001b[A\n",
      "68it [00:06,  9.10it/s]\u001b[A\n",
      "69it [00:06,  8.94it/s]\u001b[A\n",
      "70it [00:06,  8.49it/s]\u001b[A\n",
      "71it [00:06,  8.64it/s]\u001b[A\n",
      "72it [00:06,  8.75it/s]\u001b[A\n",
      "73it [00:06,  8.91it/s]\u001b[A\n",
      "74it [00:06,  8.49it/s]\u001b[A\n",
      "75it [00:07,  8.19it/s]\u001b[A\n",
      "76it [00:07,  7.74it/s]\u001b[A\n",
      "77it [00:07,  7.48it/s]\u001b[A\n",
      "78it [00:07,  7.30it/s]\u001b[A\n",
      "79it [00:07,  7.09it/s]\u001b[A\n",
      "80it [00:07,  7.04it/s]\u001b[A\n",
      "81it [00:07,  7.25it/s]\u001b[A\n",
      "82it [00:08,  7.17it/s]\u001b[A\n",
      "83it [00:08,  7.07it/s]\u001b[A\n",
      "84it [00:08,  7.19it/s]\u001b[A\n",
      "85it [00:08,  7.17it/s]\u001b[A\n",
      "86it [00:08,  7.52it/s]\u001b[A\n",
      "87it [00:08,  7.90it/s]\u001b[A\n",
      "88it [00:08,  8.03it/s]\u001b[A\n",
      "90it [00:08,  9.05it/s]\u001b[A\n",
      "92it [00:09, 10.06it/s]\u001b[A\n",
      "94it [00:09, 10.59it/s]\u001b[A\n",
      "96it [00:09, 11.22it/s]\u001b[A\n",
      "98it [00:09, 10.69it/s]\u001b[A\n",
      "100it [00:09, 10.33it/s]\u001b[A\n",
      "102it [00:10, 10.06it/s]\u001b[A\n",
      "104it [00:10,  9.83it/s]\u001b[A\n",
      "105it [00:10,  9.47it/s]\u001b[A\n",
      "106it [00:10,  9.43it/s]\u001b[A\n",
      "107it [00:10,  9.52it/s]\u001b[A\n",
      "108it [00:10,  9.59it/s]\u001b[A\n",
      "110it [00:10, 10.08it/s]\u001b[A\n",
      "112it [00:11, 10.39it/s]\u001b[A\n",
      "114it [00:11, 10.58it/s]\u001b[A\n",
      "116it [00:11, 10.38it/s]\u001b[A\n",
      "118it [00:11, 10.54it/s]\u001b[A\n",
      "120it [00:11, 10.41it/s]\u001b[A\n",
      "122it [00:12, 10.34it/s]\u001b[A\n",
      "124it [00:12, 10.02it/s]\u001b[A\n",
      "126it [00:12,  9.72it/s]\u001b[A\n",
      "127it [00:12,  9.62it/s]\u001b[A\n",
      "128it [00:12,  9.65it/s]\u001b[A\n",
      "130it [00:12,  9.83it/s]\u001b[A\n",
      "131it [00:13,  9.71it/s]\u001b[A\n",
      "132it [00:13,  9.60it/s]\u001b[A\n",
      "134it [00:13, 10.17it/s]\u001b[A\n",
      "136it [00:13, 10.48it/s]\u001b[A\n",
      "138it [00:13, 10.35it/s]\u001b[A\n",
      "140it [00:13, 10.15it/s]\u001b[A\n",
      "142it [00:14,  9.79it/s]\u001b[A\n",
      "143it [00:14,  9.81it/s]\u001b[A\n",
      "144it [00:14,  9.81it/s]\u001b[A\n",
      "146it [00:14,  9.95it/s]\u001b[A\n",
      "147it [00:14,  9.91it/s]\u001b[A\n",
      "148it [00:14,  9.82it/s]\u001b[A\n",
      "150it [00:14, 10.40it/s]\u001b[A\n",
      "152it [00:15, 10.92it/s]\u001b[A\n",
      "154it [00:15, 11.29it/s]\u001b[A\n",
      "156it [00:15, 11.10it/s]\u001b[A\n",
      "158it [00:15, 10.91it/s]\u001b[A\n",
      "160it [00:15, 10.89it/s]\u001b[A\n",
      "162it [00:15, 10.73it/s]\u001b[A\n",
      "164it [00:16, 10.49it/s]\u001b[A\n",
      "166it [00:16, 10.35it/s]\u001b[A\n",
      "168it [00:16, 10.25it/s]\u001b[A\n",
      "170it [00:16, 10.20it/s]\u001b[A\n",
      "172it [00:16, 10.10it/s]\u001b[A\n",
      "174it [00:17,  9.90it/s]\u001b[A\n",
      "175it [00:17,  9.75it/s]\u001b[A\n",
      "176it [00:17,  9.34it/s]\u001b[A\n",
      "177it [00:17,  9.01it/s]\u001b[A\n",
      "178it [00:17,  8.71it/s]\u001b[A\n",
      "179it [00:17,  8.52it/s]\u001b[A\n",
      "180it [00:17,  8.25it/s]\u001b[A\n",
      "181it [00:18,  8.05it/s]\u001b[A\n",
      "182it [00:18,  7.88it/s]\u001b[A\n",
      "183it [00:18,  7.78it/s]\u001b[A\n",
      "184it [00:18,  7.67it/s]\u001b[A\n",
      "185it [00:18,  7.50it/s]\u001b[A\n",
      "186it [00:18,  7.53it/s]\u001b[A\n",
      "187it [00:18,  7.36it/s]\u001b[A\n",
      "188it [00:19,  7.04it/s]\u001b[A\n",
      "189it [00:19,  6.95it/s]\u001b[A\n",
      "190it [00:19,  6.77it/s]\u001b[A\n",
      "191it [00:19,  6.72it/s]\u001b[A\n",
      "192it [00:19,  6.70it/s]\u001b[A\n",
      "193it [00:19,  6.70it/s]\u001b[A\n",
      "194it [00:19,  6.70it/s]\u001b[A\n",
      "195it [00:20,  6.79it/s]\u001b[A\n",
      "196it [00:20,  6.66it/s]\u001b[A\n",
      "197it [00:20,  6.45it/s]\u001b[A\n",
      "198it [00:20,  6.29it/s]\u001b[A\n",
      "199it [00:20,  6.12it/s]\u001b[A\n",
      "200it [00:20,  6.07it/s]\u001b[A\n",
      "201it [00:21,  6.03it/s]\u001b[A\n",
      "202it [00:21,  6.09it/s]\u001b[A\n",
      "203it [00:21,  6.14it/s]\u001b[A\n",
      "204it [00:21,  6.20it/s]\u001b[A\n",
      "205it [00:21,  6.29it/s]\u001b[A\n",
      "206it [00:21,  6.36it/s]\u001b[A\n",
      "207it [00:22,  6.28it/s]\u001b[A\n",
      "208it [00:22,  6.26it/s]\u001b[A\n",
      "209it [00:22,  6.15it/s]\u001b[A\n",
      "210it [00:22,  6.27it/s]\u001b[A\n",
      "211it [00:22,  6.33it/s]\u001b[A\n",
      "212it [00:22,  6.19it/s]\u001b[A\n",
      "213it [00:23,  6.22it/s]\u001b[A\n",
      "214it [00:23,  6.24it/s]\u001b[A\n",
      "215it [00:23,  6.65it/s]\u001b[A\n",
      "216it [00:23,  6.75it/s]\u001b[A\n",
      "217it [00:23,  6.81it/s]\u001b[A\n",
      "218it [00:23,  7.10it/s]\u001b[A\n",
      "219it [00:23,  6.82it/s]\u001b[A\n",
      "220it [00:24,  6.88it/s]\u001b[A\n",
      "221it [00:24,  6.90it/s]\u001b[A\n",
      "222it [00:24,  6.80it/s]\u001b[A\n",
      "223it [00:24,  6.74it/s]\u001b[A\n",
      "224it [00:24,  6.80it/s]\u001b[A\n",
      "225it [00:24,  6.99it/s]\u001b[A\n",
      "226it [00:24,  7.37it/s]\u001b[A\n",
      "227it [00:24,  7.63it/s]\u001b[A\n",
      "228it [00:25,  7.42it/s]\u001b[A\n",
      "229it [00:25,  7.59it/s]\u001b[A\n",
      "230it [00:25,  7.72it/s]\u001b[A\n",
      "231it [00:25,  7.79it/s]\u001b[A\n",
      "232it [00:25,  7.85it/s]\u001b[A\n",
      "233it [00:25,  8.03it/s]\u001b[A\n",
      "234it [00:25,  8.15it/s]\u001b[A\n",
      "235it [00:25,  8.17it/s]\u001b[A\n",
      "236it [00:26,  7.96it/s]\u001b[A\n",
      "237it [00:26,  7.96it/s]\u001b[A\n",
      "238it [00:26,  8.10it/s]\u001b[A\n",
      "239it [00:26,  8.36it/s]\u001b[A\n",
      "240it [00:26,  8.71it/s]\u001b[A\n",
      "241it [00:26,  8.99it/s]\u001b[A\n",
      "242it [00:26,  9.10it/s]\u001b[A\n",
      "243it [00:26,  9.29it/s]\u001b[A\n",
      "244it [00:26,  9.39it/s]\u001b[A\n",
      "245it [00:27,  8.95it/s]\u001b[A\n",
      "246it [00:27,  8.95it/s]\u001b[A\n",
      "247it [00:27,  8.91it/s]\u001b[A\n",
      "248it [00:27,  8.93it/s]\u001b[A\n",
      "249it [00:27,  8.94it/s]\u001b[A\n",
      "250it [00:27,  9.14it/s]\u001b[A\n",
      "251it [00:27,  8.89it/s]\u001b[A\n",
      "252it [00:27,  8.73it/s]\u001b[A\n",
      "253it [00:28,  8.82it/s]\u001b[A\n",
      "254it [00:28,  8.70it/s]\u001b[A\n",
      "255it [00:28,  8.62it/s]\u001b[A\n",
      "256it [00:28,  8.55it/s]\u001b[A\n",
      "257it [00:28,  8.50it/s]\u001b[A\n",
      "258it [00:28,  8.27it/s]\u001b[A\n",
      "259it [00:28,  8.11it/s]\u001b[A\n",
      "260it [00:28,  7.87it/s]\u001b[A\n",
      "261it [00:29,  7.88it/s]\u001b[A\n",
      "262it [00:29,  8.04it/s]\u001b[A\n",
      "263it [00:29,  8.15it/s]\u001b[A\n",
      "264it [00:29,  8.20it/s]\u001b[A\n",
      "265it [00:29,  8.24it/s]\u001b[A\n",
      "266it [00:29,  8.20it/s]\u001b[A\n",
      "267it [00:29,  8.26it/s]\u001b[A\n",
      "268it [00:29,  8.47it/s]\u001b[A\n",
      "269it [00:29,  8.76it/s]\u001b[A\n",
      "270it [00:30,  8.95it/s]\u001b[A\n",
      "271it [00:30,  9.20it/s]\u001b[A\n",
      "272it [00:30,  9.20it/s]\u001b[A\n",
      "273it [00:30,  9.04it/s]\u001b[A\n",
      "274it [00:30,  8.82it/s]\u001b[A\n",
      "275it [00:30,  8.70it/s]\u001b[A\n",
      "276it [00:30,  8.61it/s]\u001b[A\n",
      "277it [00:30,  8.57it/s]\u001b[A\n",
      "278it [00:30,  8.30it/s]\u001b[A\n",
      "279it [00:31,  8.12it/s]\u001b[A\n",
      "280it [00:31,  8.02it/s]\u001b[A\n",
      "281it [00:31,  7.98it/s]\u001b[A\n",
      "282it [00:31,  7.97it/s]\u001b[A\n",
      "283it [00:31,  8.08it/s]\u001b[A\n",
      "284it [00:31,  8.15it/s]\u001b[A\n",
      "285it [00:31,  8.17it/s]\u001b[A\n",
      "286it [00:31,  8.21it/s]\u001b[A\n",
      "287it [00:32,  8.06it/s]\u001b[A\n",
      "288it [00:32,  8.30it/s]\u001b[A\n",
      "289it [00:32,  8.49it/s]\u001b[A\n",
      "290it [00:32,  8.30it/s]\u001b[A\n",
      "291it [00:32,  8.49it/s]\u001b[A\n",
      "292it [00:32,  8.62it/s]\u001b[A\n",
      "293it [00:32,  8.73it/s]\u001b[A\n",
      "294it [00:32,  8.79it/s]\u001b[A\n",
      "295it [00:33,  8.86it/s]\u001b[A\n",
      "296it [00:33,  8.45it/s]\u001b[A\n",
      "297it [00:33,  8.03it/s]\u001b[A\n",
      "298it [00:33,  8.12it/s]\u001b[A\n",
      "299it [00:33,  8.92it/s]\u001b[A\n",
      "9it [00:37,  4.17s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m mv_enc\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m# Enable inference\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, (_, _, imgs, imgs_sz, _, track_idxs, _, _, _) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(val_dl)):\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# pcls = pcls.to(model_device)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(model_device)\n\u001b[1;32m     40\u001b[0m         track_idxs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(track_idxs\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32))\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.10/site-packages/torch/utils/data/dataloader.py:635\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.10/site-packages/torch/utils/data/dataloader.py:679\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    678\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    681\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Research/MOT-Research/CLORT/clort/data/ArgoCL.py:245\u001b[0m, in \u001b[0;36mArgoCL.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Aggregate informations from all frames in temporal horizon\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpc:\n\u001b[0;32m--> 245\u001b[0m     pcls \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m (pivot \u001b[38;5;28;01mif\u001b[39;00m pivot \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)) \u001b[38;5;66;03m# Concatenate on points dimension\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim:\n\u001b[1;32m    248\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mconcatenate(imgs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.\u001b[39m) \u001b[38;5;66;03m# Concatenate images on channel dimension # [_, 250, 250]\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "for epoch in range(last_epoch+1, n_epochs):\n",
    "    model_path = os.path.join(model_save_dir, f'model_{epoch}.pth')\n",
    "\n",
    "    # Training loop\n",
    "    for itr, (_, _, imgs, imgs_sz, _, track_idxs, _, _, _) in tqdm(enumerate(train_dl)):\n",
    "        mv_enc.train() # Enable training\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # pcls = pcls.to(model_device)\n",
    "        imgs = imgs.to(model_device)\n",
    "        track_idxs = torch.from_numpy(track_idxs.astype(np.int32))\n",
    "        # bboxs = bboxs.to(model_device)\n",
    "\n",
    "        mv_e = mv_enc(imgs, imgs_sz)\n",
    "\n",
    "        loss = cl(mv_e, track_idxs, mb.get_memory())\n",
    "        training_loss.append(loss.numpy(force=True).item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr%10 == 9:\n",
    "#             print(f'{epoch = } and {itr = } : Mean Training loss : {np.mean(training_loss[-10:])}')\n",
    "\n",
    "    ###################################################################################\n",
    "            ### Validation loss\n",
    "            mb_infer = MemoryBankInfer(val_dataset.n_tracks, n_features, 5, 3, 'cpu')\n",
    "\n",
    "            cl_infer = ContrastiveLoss(static_contrast=False)\n",
    "\n",
    "            val_loss = 0.0\n",
    "\n",
    "            mv_enc.eval() # Enable inference\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for _, (_, _, imgs, imgs_sz, _, track_idxs, _, _, _) in enumerate(val_dl):\n",
    "                    # pcls = pcls.to(model_device)\n",
    "                    imgs = imgs.to(model_device)\n",
    "                    track_idxs = torch.from_numpy(track_idxs.astype(np.int32))\n",
    "                    # bboxs = bboxs.to(model_device)\n",
    "\n",
    "                    mv_e : torch.Tensor = mv_enc(imgs, imgs_sz)\n",
    "                    loss : torch.Tensor = cl_infer(mv_e, track_idxs, mb_infer.get_memory())\n",
    "\n",
    "                    val_loss += loss.detach().cpu().item()\n",
    "\n",
    "                    mb_infer.update(mv_e.detach().cpu(), track_idxs)\n",
    "\n",
    "            val_loss /= len(val_dl)\n",
    "            validation_loss.append(val_loss)\n",
    "\n",
    "#             print(f'{epoch = } and {itr = } : Mean Validation loss : {val_loss = }')\n",
    "\n",
    "            wandb.log({'epoch': epoch+1, 'itr': itr+1,\n",
    "                        'training_loss': np.mean(training_loss[-10:]),\n",
    "                        'val_loss': val_loss})\n",
    "            ### Validation loss\n",
    "    ###################################################################################\n",
    "    lr_scheduler.step() # Step Learning rate\n",
    "\n",
    "    model_info = {\n",
    "        'EPOCH': epoch,\n",
    "        'mv_enc': mv_enc.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'train_loss': training_loss,\n",
    "        'val_loss': validation_loss\n",
    "    }\n",
    "\n",
    "    torch.save(model_info, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
