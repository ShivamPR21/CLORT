{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff1c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505733b6",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c4de257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from clort.clearn.data.cltracking import ContrastiveLearningTracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa6ffd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:argoverse.data_loading.synchronization_database:No corresponding stereo image at 315967661819717000: 176.11195999999998 > 100.0 ms\n",
      "WARNING:argoverse.data_loading.synchronization_database:No corresponding stereo image at 315967661819717000: 176.107928 > 100.0 ms\n"
     ]
    }
   ],
   "source": [
    "root = '../../../datasets/argoverse-tracking/train1/'\n",
    "dataset = ContrastiveLearningTracking(root, \n",
    "                                      occlusion_thresh = 30., \n",
    "                                      central_crop=True, \n",
    "                                      img_tr_ww = (0.9, 0.9), \n",
    "                                      image_size_threshold=100,\n",
    "                                      img_reshape = (256, 256),\n",
    "                                      ids_repeat=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25668b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dataset_init(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89b0bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbede99",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d03d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clort.clearn.models import Level1Encoder, PointCloudEncoder, FeatureMixer\n",
    "from mzLosses.contrastive import SoftNearestNeighbourLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7814ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_model = Level1Encoder()\n",
    "pcl_model = PointCloudEncoder(10)\n",
    "feature_mixer = FeatureMixer(vis_size = 512, pcl_size = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1840ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "728b5844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureMixer(\n",
       "  (activation): SELU()\n",
       "  (v_linear_1): Linear(in_features=512, out_features=128, bias=False)\n",
       "  (v_bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (v_linear_2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (f_linear_1): Linear(in_features=124, out_features=128, bias=True)\n",
       "  (f_linear_2): Linear(in_features=128, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_model.to(device)\n",
    "pcl_model.to(device)\n",
    "feature_mixer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49b1e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SoftNearestNeighbourLoss()\n",
    "vis_optim = torch.optim.AdamW(vis_model.parameters())\n",
    "pcl_optim = torch.optim.AdamW(pcl_model.parameters())\n",
    "feature_optim = torch.optim.AdamW(feature_mixer.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab8382fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7799], grad_fn=<DivBackward0>)\n",
      "tensor([9.5268], grad_fn=<DivBackward0>)\n",
      "tensor([2.5566], grad_fn=<DivBackward0>)\n",
      "tensor([10.5919], grad_fn=<DivBackward0>)\n",
      "tensor([1.1632], grad_fn=<DivBackward0>)\n",
      "tensor([4.4903], grad_fn=<DivBackward0>)\n",
      "tensor([2.8962], grad_fn=<DivBackward0>)\n",
      "tensor([2.4405], grad_fn=<DivBackward0>)\n",
      "tensor([1.3993], grad_fn=<DivBackward0>)\n",
      "tensor([0.9747], grad_fn=<DivBackward0>)\n",
      "tensor([-0.], grad_fn=<DivBackward0>)\n",
      "tensor([1.1178], grad_fn=<DivBackward0>)\n",
      "tensor([1.2305], grad_fn=<DivBackward0>)\n",
      "tensor([1.2367], grad_fn=<DivBackward0>)\n",
      "tensor([7.2398], grad_fn=<DivBackward0>)\n",
      "tensor([1.1998], grad_fn=<DivBackward0>)\n",
      "tensor([1.7643], grad_fn=<DivBackward0>)\n",
      "tensor([0.9517], grad_fn=<DivBackward0>)\n",
      "tensor([1.6349], grad_fn=<DivBackward0>)\n",
      "tensor([0.8895], grad_fn=<DivBackward0>)\n",
      "tensor([1.2201], grad_fn=<DivBackward0>)\n",
      "tensor([1.1448], grad_fn=<DivBackward0>)\n",
      "tensor([0.9429], grad_fn=<DivBackward0>)\n",
      "tensor([1.4422], grad_fn=<DivBackward0>)\n",
      "tensor([4.1984], grad_fn=<DivBackward0>)\n",
      "tensor([1.9454], grad_fn=<DivBackward0>)\n",
      "tensor([0.9130], grad_fn=<DivBackward0>)\n",
      "tensor([1.0103], grad_fn=<DivBackward0>)\n",
      "tensor([1.0536], grad_fn=<DivBackward0>)\n",
      "tensor([2.0721], grad_fn=<DivBackward0>)\n",
      "tensor([3.3101], grad_fn=<DivBackward0>)\n",
      "tensor([0.9072], grad_fn=<DivBackward0>)\n",
      "tensor([1.2833], grad_fn=<DivBackward0>)\n",
      "tensor([2.3031], grad_fn=<DivBackward0>)\n",
      "tensor([0.8168], grad_fn=<DivBackward0>)\n",
      "tensor([1.1388], grad_fn=<DivBackward0>)\n",
      "tensor([1.0483], grad_fn=<DivBackward0>)\n",
      "tensor([1.2272], grad_fn=<DivBackward0>)\n",
      "tensor([1.6733], grad_fn=<DivBackward0>)\n",
      "tensor([0.9141], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     imgs_enc \u001b[38;5;241m=\u001b[39m vis_model(imgs)\n\u001b[1;32m     13\u001b[0m     pcls_enc \u001b[38;5;241m=\u001b[39m pcl_model(pcls)\n\u001b[0;32m---> 14\u001b[0m     final_enc \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_mixer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcls_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     print(final_enc.size())\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(final_enc, track_ids)\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.8/site-packages/torch/nn/modules/module.py:1129\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1128\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Research/MOT-Research/CLORT/clort/clearn/models/feature_mixing.py:27\u001b[0m, in \u001b[0;36mFeatureMixer.forward\u001b[0;34m(self, x_vis, x_pcl)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_vis: torch\u001b[38;5;241m.\u001b[39mTensor, x_pcl: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 27\u001b[0m     x_vis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_bn_1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_linear_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_vis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     28\u001b[0m     x_vis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_linear_2(x_vis))\n\u001b[1;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x_vis, x_pcl), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.8/site-packages/torch/nn/modules/module.py:1129\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1128\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/clort/lib/python3.8/site-packages/torch/nn/functional.py:2387\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2385\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2388\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2389\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for imgs, pcls, track_ids in dl:\n",
    "    b, n_view, C, H, W = imgs.size()\n",
    "    _, _, d, n = pcls.size()\n",
    "    \n",
    "    imgs, pcls, track_ids = imgs.view(b*n_view, C, H, W), pcls.view(b*n_view, d, n), track_ids.flatten()\n",
    "    imgs, pcls, track_ids = imgs.to(device), pcls.to(device), track_ids.to(device)\n",
    "    \n",
    "    vis_optim.zero_grad()\n",
    "    pcl_optim.zero_grad()\n",
    "    feature_optim.zero_grad()\n",
    "    \n",
    "    imgs_enc = vis_model(imgs)\n",
    "    pcls_enc = pcl_model(pcls)\n",
    "    final_enc = feature_mixer(imgs_enc, pcls_enc)\n",
    "    \n",
    "#     print(final_enc.size())\n",
    "    loss = criterion(final_enc, track_ids)\n",
    "    print(loss)\n",
    "    \n",
    "    loss.backward()\n",
    "    vis_optim.step()\n",
    "    pcl_optim.step()\n",
    "    feature_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc0a8c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "757dae30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 60])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcls_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d53312f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 3, 30])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "443d9de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  1,  1,  1, 19, 19, 19, 19, 19,  8,  8,  8,  8,  8,  3,  3,  3,\n",
       "         3,  3, 23, 23, 23, 23, 23, 19, 19, 19, 19, 19], dtype=torch.int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272edd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
